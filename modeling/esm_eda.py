#!/usr/bin/env python3
"""
Compare baseline component models with and without scaffold ESM entropy features.

This script:
1. Loads the existing profiling features CSV.
2. Loads ESM context features computed by compute_esm_context.py.
3. Merges them on sample_id.
4. For each component and target, trains Ridge and RandomForest models on
   baseline features vs augmented features (baseline + ESM entropy).
5. Prints R^2 and deltas, and saves comparison plots per component.
"""

import argparse
import os
from pathlib import Path
from typing import Any, Dict, List, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import spearmanr
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import GroupShuffleSplit, train_test_split
from sklearn.metrics import r2_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline


DEFAULT_FEATURES_CSV = "modeling/features.csv"
DEFAULT_ESM_CSV = "modeling/esm_context_features.csv"
DEFAULT_OUTDIR = "modeling/results"
DEFAULT_REPORT_TXT = "modeling/results/esm_fit_report.txt"
MIN_ROWS = 20
MIN_NONNULL_FRAC = 0.5
TEST_SIZE = 0.2
RANDOM_STATE = 0
ESM_FEATURE_COLUMNS = [
    "esm_mean_entropy",
    "esm_max_entropy",
    "esm_std_entropy",
    "esm_mean_margin",
    "esm_min_margin",
    "esm_std_margin",
]
BASELINE_AUGMENT_MODES = ("none", "length", "length_steps", "compute_proxy")
DEFAULT_LOG1P_BASE_COLUMNS = ("scaffold_length", "ligand_length", "output_samples")
STEP_LIKE_PATTERNS = ("_step", "_steps", "num_designs")
TARGETS: Sequence[Tuple[str, str]] = (
    ("log_runtime_sec", "Log runtime (sec)"),
    ("log_peak_memory_mib", "Log peak memory (MiB)"),
    ("mean_temporal_util_percent", "Mean temporal util (%)"),
)
MODEL_BUILDERS = {
    "Ridge": lambda: Ridge(alpha=1.0),
    "Random Forest": lambda: RandomForestRegressor(
        n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1
    ),
}
SPLIT_MODES = ("row", "group", "both")
ALL_EXPERIMENTS_INTRO = """\
This report was generated by --run-all-experiments.

Goal (based on our discussion):
1) Check if ESM entropy improvements are real group-generalization signals,
   not sample_id leakage or row-heavy-group artifacts.
2) Test whether ESM is proxying hidden baseline scaling (length / compute terms).
3) Compare raw ESM vs low-dimensional PCA ESM.
4) Stabilize tree baselines in small-group settings (ExtraTrees vs RF).

Each experiment block below prints its intent, key settings, and results.

Tip: pass --report-details to include per-split coefficient/importance dumps
(this can make the report very long, especially with many group splits).
"""


def build_all_experiment_presets(
    args: argparse.Namespace,
) -> List[Dict[str, object]]:
    base_outdir = Path(args.outdir)
    return [
        {
            "title": "E1 Ridge + length baseline + raw ESM (group CV)",
            "description": (
                "Intent: baseline에 길이 스케일링(log1p(lengths/output_samples), "
                "scaffold_length^2)을 넣고, ESM raw 6개를 추가했을 때 "
                "group split에서 남는 delta가 있는지 확인. "
                "즉 ESM이 단순 길이 proxy인지 점검."
            ),
            "overrides": {
                "split_mode": "group",
                "group_n_splits": 10,
                "baseline_augment": "length",
                "standardize_linear": True,
                "esm_pca_components": 0,
                "group_metrics": False,
                "permute_esm_n": 0,
                "include_extra_trees": False,
            },
            "outdir": str(base_outdir / "E1_ridge_length_raw"),
        },
        {
            "title": "E2 Ridge + length baseline + ESM PCA2 (group CV)",
            "description": (
                "Intent: ESM 6개를 PCA 2차원으로 줄여도 raw 대비 신호를 유지하는지, "
                "그리고 group split 일반화 delta가 남는지 확인."
            ),
            "overrides": {
                "split_mode": "group",
                "group_n_splits": 10,
                "baseline_augment": "length",
                "standardize_linear": True,
                "esm_pca_components": 2,
                "group_metrics": False,
                "permute_esm_n": 0,
                "include_extra_trees": False,
            },
            "outdir": str(base_outdir / "E2_ridge_length_pca2"),
        },
        {
            "title": "E3 ExtraTrees tuned + length baseline + ESM PCA2 (group CV)",
            "description": (
                "Intent: small-group/row 반복 조건에서 RF가 불안정한 패턴을 보였기 때문에, "
                "분산이 낮은 ExtraTrees를 같은 조건(min_samples_leaf↑, max_depth 제한)으로 "
                "비교. tree 계열에서도 ESM(PCA2) 추가 이득이 남는지 확인."
            ),
            "overrides": {
                "split_mode": "group",
                "group_n_splits": 5,
                "baseline_augment": "length",
                "standardize_linear": False,
                "esm_pca_components": 2,
                "tree_n_estimators": 100,
                "tree_min_samples_leaf": 5,
                "tree_max_depth": 8,
                "include_extra_trees": True,
                "group_metrics": False,
                "permute_esm_n": 0,
            },
            "outdir": str(base_outdir / "E3_extratrees_pca2"),
        },
        {
            "title": "E4 rfdiffusion conservative check (group-balanced + perm-null)",
            "description": (
                "Intent: rfdiffusion에서 남은 ESM 개선이 '그룹 간 일반화' 신호인지 "
                "최대한 보수적으로 검증. "
                "group-mean R2/Spearman, group-balanced MSE를 계산하고, "
                "train 내 ESM을 permutation하여 null delta 분포와 비교."
            ),
            "overrides": {
                "components": ["rfdiffusion"],
                "split_mode": "group",
                "group_n_splits": 50,
                "baseline_augment": "length",
                "standardize_linear": True,
                "esm_pca_components": 2,
                "group_metrics": True,
                "permute_esm_n": 20,
                "permute_seed": 0,
                "tree_n_estimators": 50,
                "tree_min_samples_leaf": 5,
                "tree_max_depth": 8,
            },
            "outdir": str(base_outdir / "E4_rfdiffusion_conservative"),
        },
        {
            "title": "E5 rfdiffusion compute_proxy baseline + PCA2 meaning",
            "description": (
                "Intent: length_steps처럼 차원을 많이 늘리지 않고, "
                "compute_proxy=log1p(length^2*step*num_designs) 1개로 "
                "숨은 스케일링 항을 직접 치환. "
                "그 상태에서도 ESM(PCA2) 개선이 남는지와, "
                "PCA loadings/잔차 상관으로 2D 의미를 해석."
            ),
            "overrides": {
                "components": ["rfdiffusion"],
                "split_mode": "group",
                "group_n_splits": 5,
                "baseline_augment": "compute_proxy",
                "standardize_linear": True,
                "esm_pca_components": 2,
                "pca_analysis": True,
                "group_metrics": False,
                "permute_esm_n": 0,
            },
            "outdir": str(base_outdir / "E5_rfdiffusion_compute_proxy"),
        },
    ]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Compare baseline vs ESM-augmented models per component."
    )
    parser.add_argument(
        "--features-csv",
        default=DEFAULT_FEATURES_CSV,
        help="Path to the baseline features CSV.",
    )
    parser.add_argument(
        "--esm-csv",
        default=DEFAULT_ESM_CSV,
        help="Path to the ESM context features CSV.",
    )
    parser.add_argument(
        "--outdir",
        default=DEFAULT_OUTDIR,
        help="Directory to store per-component plots.",
    )
    parser.add_argument(
        "--report-txt",
        default=DEFAULT_REPORT_TXT,
        help="Path to write a pretty-printed text summary of model fits.",
    )
    report_detail_group = parser.add_mutually_exclusive_group()
    report_detail_group.add_argument(
        "--report-details",
        dest="report_details",
        action="store_true",
        help="Include per-split coefficient/importance dumps in the report.",
    )
    report_detail_group.add_argument(
        "--no-report-details",
        dest="report_details",
        action="store_false",
        help="Disable per-split coefficient/importance dumps in the report.",
    )
    report_detail_group.set_defaults(report_details=None)
    parser.add_argument(
        "--top-features",
        type=int,
        default=20,
        help="How many top features (by |coef| or importance) to include per model; "
        "set <=0 to include all.",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print per-fit R^2 lines during training (otherwise only summaries).",
    )
    parser.add_argument(
        "--min-rows",
        type=int,
        default=MIN_ROWS,
        help="Minimum rows required per component to run modeling.",
    )
    parser.add_argument(
        "--min-nonnull-frac",
        type=float,
        default=MIN_NONNULL_FRAC,
        help="Drop feature columns with lower non-null coverage per component.",
    )
    parser.add_argument(
        "--components",
        nargs="*",
        default=None,
        help="Optional list of component names to run (default: all).",
    )
    parser.add_argument(
        "--split-mode",
        choices=SPLIT_MODES,
        default="both",
        help=(
            "How to split train/test. "
            "'row' = random row split; "
            "'group' = GroupShuffleSplit by sample_id; "
            "'both' = run both for leakage check."
        ),
    )
    parser.add_argument(
        "--group-n-splits",
        type=int,
        default=1,
        help=(
            "How many GroupShuffleSplit repetitions to run when split-mode includes "
            "'group'. Results are averaged in summaries."
        ),
    )
    parser.add_argument(
        "--baseline-augment",
        choices=BASELINE_AUGMENT_MODES,
        default="none",
        help=(
            "Add a small set of nonlinear baseline features to test whether ESM gains "
            "are proxying hidden scaling terms. "
            "'length' adds log1p(lengths/output_samples) and scaffold_length^2. "
            "'length_steps' additionally adds log1p(step/num_designs-like columns) "
            "and one length^2 x step interaction. "
            "'compute_proxy' adds a single log1p(length^2 * step * num_designs) "
            "proxy when the needed columns exist."
        ),
    )
    parser.add_argument(
        "--tree-n-estimators",
        type=int,
        default=200,
        help="Number of trees for RandomForest/ExtraTrees models.",
    )
    parser.add_argument(
        "--tree-min-samples-leaf",
        type=int,
        default=1,
        help="min_samples_leaf for tree models; increase to reduce group overfitting.",
    )
    parser.add_argument(
        "--tree-max-depth",
        type=int,
        default=0,
        help="max_depth for tree models; set <=0 for unlimited depth.",
    )
    parser.add_argument(
        "--include-extra-trees",
        action="store_true",
        help="Also fit ExtraTreesRegressor (lower-variance tree baseline).",
    )
    parser.add_argument(
        "--esm-pca-components",
        type=int,
        default=0,
        help=(
            "If >0, reduce the 6 ESM features to this many PCA components "
            "(PCA fitted on train split only) before augmentation."
        ),
    )
    parser.add_argument(
        "--pca-analysis",
        action="store_true",
        help=(
            "For components run, print ESM PCA loadings and correlations between "
            "PC scores and baseline residuals."
        ),
    )
    parser.add_argument(
        "--standardize-linear",
        action="store_true",
        help="Standardize features for Ridge (helps stability with nonlinear baseline terms).",
    )
    parser.add_argument(
        "--group-metrics",
        action="store_true",
        help=(
            "Also compute conservative group-balanced metrics on test splits: "
            "group-mean R2/Spearman and group-balanced MSE."
        ),
    )
    parser.add_argument(
        "--permute-esm-n",
        type=int,
        default=0,
        help=(
            "If >0, run a permutation null test by shuffling ESM features within "
            "train for this many permutations per split."
        ),
    )
    parser.add_argument(
        "--permute-seed",
        type=int,
        default=0,
        help="Base random seed for ESM permutation null tests.",
    )
    parser.add_argument(
        "--run-all-experiments",
        action="store_true",
        help=(
            "Run the preset experiment suite from our discussion and write a single "
            "combined report to --report-txt. Preset overrides replace many modeling "
            "flags (e.g., split_mode/group_n_splits); report formatting flags like "
            "--report-details still apply."
        ),
    )
    return parser.parse_args()


def resolve_report_details(args: argparse.Namespace) -> bool:
    if args.report_details is None:
        return not args.run_all_experiments
    return bool(args.report_details)


def load_baseline_features(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    df = df.dropna(subset=["runtime_sec", "peak_memory_mib", "sample_id"])
    df = df[(df["runtime_sec"] > 0) & (df["peak_memory_mib"] > 0)]
    return df


def select_baseline_feature_columns(df: pd.DataFrame) -> List[str]:
    numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    target_cols = [
        "runtime_sec",
        "peak_memory_mib",
        "mean_temporal_util_percent",
        "log_runtime_sec",
        "log_peak_memory_mib",
    ]
    return [col for col in numeric_cols if col not in target_cols]


def filter_features_by_nonnull(
    df: pd.DataFrame, feature_cols: Sequence[str], min_nonnull_frac: float
) -> List[str]:
    kept: List[str] = []
    for col in feature_cols:
        frac = df[col].notna().mean()
        if frac < min_nonnull_frac:
            continue
        if df[col].nunique(dropna=True) <= 1:
            continue
        kept.append(col)
    return kept


def add_derived_baseline_features(
    df_comp: pd.DataFrame,
    baseline_cols: Sequence[str],
    augment_mode: str,
) -> List[str]:
    if augment_mode == "none":
        return []
    derived_cols: List[str] = []

    if augment_mode == "compute_proxy":
        if "scaffold_length" in df_comp.columns:
            step_cols = [
                col for col in baseline_cols if any(pat in col for pat in ("_step", "_steps"))
            ]
            design_cols = [
                col for col in baseline_cols if "num_designs" in col
            ]
            if step_cols and design_cols:
                step_col = step_cols[0]
                design_col = design_cols[0]
                df_comp["compute_proxy"] = np.log1p(
                    (df_comp["scaffold_length"] ** 2)
                    * df_comp[step_col]
                    * df_comp[design_col]
                )
                derived_cols.append("compute_proxy")
        return derived_cols

    if augment_mode in ("length", "length_steps"):
        for col in DEFAULT_LOG1P_BASE_COLUMNS:
            if col in df_comp.columns:
                new_col = f"log1p_{col}"
                df_comp[new_col] = np.log1p(df_comp[col])
                derived_cols.append(new_col)
        if "scaffold_length" in df_comp.columns:
            df_comp["scaffold_length_sq"] = df_comp["scaffold_length"] ** 2
            derived_cols.append("scaffold_length_sq")

    if augment_mode == "length_steps":
        step_cols = [
            col
            for col in baseline_cols
            if any(pat in col for pat in STEP_LIKE_PATTERNS)
        ]
        for col in step_cols:
            new_col = f"log1p_{col}"
            if new_col in df_comp.columns:
                continue
            df_comp[new_col] = np.log1p(df_comp[col])
            derived_cols.append(new_col)

        if "scaffold_length_sq" in df_comp.columns and step_cols:
            step_col = step_cols[0]
            inter_col = f"scaffold_length_sq_x_{step_col}"
            df_comp[inter_col] = df_comp["scaffold_length_sq"] * df_comp[step_col]
            derived_cols.append(inter_col)

    return derived_cols


def make_model_builders(args: argparse.Namespace) -> Dict[str, Any]:
    max_depth = None if args.tree_max_depth <= 0 else args.tree_max_depth
    if args.standardize_linear:
        ridge_builder = lambda: make_pipeline(
            StandardScaler(), Ridge(alpha=1.0)
        )
    else:
        ridge_builder = lambda: Ridge(alpha=1.0)
    builders: Dict[str, Any] = {
        "Ridge": ridge_builder,
        "Random Forest": lambda: RandomForestRegressor(
            n_estimators=args.tree_n_estimators,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            min_samples_leaf=args.tree_min_samples_leaf,
            max_depth=max_depth,
        ),
    }
    if args.include_extra_trees:
        builders["Extra Trees"] = lambda: ExtraTreesRegressor(
            n_estimators=args.tree_n_estimators,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            min_samples_leaf=args.tree_min_samples_leaf,
            max_depth=max_depth,
        )
    return builders


def run_pca_analysis(
    component: str,
    df_comp: pd.DataFrame,
    baseline_cols: Sequence[str],
    n_components: int,
) -> List[str]:
    if n_components <= 0 or df_comp.empty:
        return []
    lines: List[str] = [f"[{component}] ESM PCA analysis:"]
    esm_matrix = df_comp[ESM_FEATURE_COLUMNS].to_numpy(dtype=float)
    scaler = StandardScaler()
    esm_scaled = scaler.fit_transform(esm_matrix)
    n_components_eff = min(
        n_components, esm_scaled.shape[0], len(ESM_FEATURE_COLUMNS)
    )
    pca = PCA(n_components=n_components_eff, random_state=RANDOM_STATE)
    pcs = pca.fit_transform(esm_scaled)
    pc_cols = [f"PC{i + 1}" for i in range(n_components_eff)]
    pc_df = pd.DataFrame(pcs, index=df_comp.index, columns=pc_cols)

    for i, pc_name in enumerate(pc_cols):
        parts = [
            f"{feat}={pca.components_[i, j]:+0.3f}"
            for j, feat in enumerate(ESM_FEATURE_COLUMNS)
        ]
        lines.append(f"[{component}] {pc_name} loadings: " + ", ".join(parts))

    for target_col, target_label in TARGETS:
        cols_needed = [target_col, *baseline_cols]
        df_t = df_comp.dropna(subset=cols_needed)
        if df_t.empty:
            continue
        X_t = df_t[list(baseline_cols)]
        y_t = df_t[target_col]
        ridge_builder = MODEL_BUILDERS.get("Ridge")
        model = ridge_builder() if ridge_builder else Ridge(alpha=1.0)
        model.fit(X_t, y_t)
        residual = y_t - model.predict(X_t)
        corrs = []
        for pc_name in pc_cols:
            rho = spearmanr(
                residual.to_numpy(),
                pc_df.loc[df_t.index, pc_name].to_numpy(),
            ).correlation
            corrs.append(f"{pc_name} rho={rho:+.3f}")
        lines.append(
            f"[{component}] baseline residual vs PCs ({target_label}): "
            + ", ".join(corrs)
        )
    return lines


def train_and_score_model(
    model_name: str,
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series,
) -> Tuple[float, Any]:
    model = MODEL_BUILDERS[model_name]()
    model.fit(X_train, y_train)
    return model.score(X_test, y_test), model


def compute_group_metrics(
    y_true: pd.Series,
    y_pred: np.ndarray,
    groups: pd.Series,
) -> Dict[str, float]:
    df_eval = pd.DataFrame(
        {"y_true": y_true.to_numpy(), "y_pred": y_pred, "group": groups.to_numpy()},
        index=y_true.index,
    )
    grouped = df_eval.groupby("group", sort=False).mean(numeric_only=True)
    metrics: Dict[str, float] = {}
    if grouped.shape[0] >= 2:
        metrics["group_mean_r2"] = float(
            r2_score(grouped["y_true"], grouped["y_pred"])
        )
        metrics["group_mean_spearman"] = float(
            spearmanr(grouped["y_true"], grouped["y_pred"]).correlation
        )
    else:
        metrics["group_mean_r2"] = float("nan")
        metrics["group_mean_spearman"] = float("nan")

    df_eval["sq_err"] = (df_eval["y_true"] - df_eval["y_pred"]) ** 2
    mse_per_group = (
        df_eval.groupby("group", sort=False)["sq_err"]
        .mean()
        .to_numpy(dtype=float)
    )
    metrics["group_balanced_mse"] = float(np.mean(mse_per_group))
    return metrics


def plot_r2_comparison(
    component: str, results: pd.DataFrame, outdir: str, split_family: str
) -> None:
    Path(outdir).mkdir(parents=True, exist_ok=True)
    if "split_family" in results.columns:
        results = results[results["split_family"] == split_family]
    elif "split" in results.columns:
        results = results[results["split"] == split_family]
    n_targets = len(TARGETS)
    fig, axes = plt.subplots(
        1, n_targets, figsize=(4 * n_targets, 4), squeeze=False
    )
    width = 0.35
    x = np.arange(len(MODEL_BUILDERS))
    for idx, (target_col, target_label) in enumerate(TARGETS):
        ax = axes[0][idx]
        subset = results[results["target_col"] == target_col]
        baseline_scores = []
        augmented_scores = []
        for model_name in MODEL_BUILDERS:
            df_model = subset[subset["model"] == model_name]
            base_r2 = df_model[df_model["feature_set"] == "baseline"]["r2"].mean()
            aug_r2 = df_model[df_model["feature_set"] == "augmented"]["r2"].mean()
            baseline_scores.append(base_r2)
            augmented_scores.append(aug_r2)

        ax.bar(x - width / 2, baseline_scores, width=width, label="Baseline")
        ax.bar(x + width / 2, augmented_scores, width=width, label="Augmented")
        ax.set_xticks(x)
        ax.set_xticklabels(list(MODEL_BUILDERS.keys()))
        ax.set_ylabel("R^2")
        ax.set_title(f"{component} | {target_label}")
        ax.legend()
        ax.grid(True, axis="y", linestyle="--", alpha=0.5)

    plt.tight_layout()
    outfile = Path(outdir) / f"{component}_r2_comparison_{split_family}.png"
    plt.savefig(outfile, dpi=150)
    plt.close(fig)
    print(f"[{component}] Saved R^2 comparison plot ({split_family}) to {outfile}")


def format_component_lines(
    component: str, results: pd.DataFrame, console: bool
) -> List[str]:
    split_families = (
        sorted(results["split_family"].dropna().unique().tolist())
        if "split_family" in results.columns
        else (
            sorted(results["split"].dropna().unique().tolist())
            if "split" in results.columns
            else ["row"]
        )
    )
    lines: List[str] = []
    for split_family in split_families:
        if "split_family" in results.columns:
            df_split = results[results["split_family"] == split_family]
        elif "split" in results.columns:
            df_split = results[results["split"] == split_family]
        else:
            df_split = results
        header = (
            f"\n=== Component: {component} | split={split_family} ==="
            if console
            else f"Component: {component} | split={split_family}"
        )
        lines.append(header)
        for target_col, target_label in TARGETS:
            df_target = df_split[df_split["target_col"] == target_col]
            if df_target.empty:
                msg = (
                    f"[{component}] {target_label}: no results."
                    if console
                    else f"  {target_label}: no results."
                )
                lines.append(msg)
                continue
            if not console:
                lines.append(f"  {target_label}:")
            for model_name in MODEL_BUILDERS:
                df_model = df_target[df_target["model"] == model_name]
                if df_model.empty:
                    continue
                pivot = df_model.pivot_table(
                    index="split", columns="feature_set", values="r2", aggfunc="mean"
                )
                base_vals = pivot.get("baseline")
                aug_vals = pivot.get("augmented")
                if base_vals is None or aug_vals is None:
                    continue
                delta_vals = aug_vals - base_vals
                n_splits = int(delta_vals.dropna().shape[0])
                base_mean = float(base_vals.mean())
                aug_mean = float(aug_vals.mean())
                delta_mean = float(delta_vals.mean())
                if n_splits > 1:
                    base_std = float(base_vals.std(ddof=0))
                    aug_std = float(aug_vals.std(ddof=0))
                    delta_std = float(delta_vals.std(ddof=0))
                    if console:
                        lines.append(
                            f"[{component}] {target_label} | {model_name}: "
                            f"baseline R^2={base_mean:.3f}±{base_std:.3f}, "
                            f"augmented R^2={aug_mean:.3f}±{aug_std:.3f}, "
                            f"delta={delta_mean:+.3f}±{delta_std:.3f} (n={n_splits})"
                        )
                    else:
                        lines.append(
                            f"    {model_name:<14} baseline={base_mean:.3f}±{base_std:.3f} "
                            f"augmented={aug_mean:.3f}±{aug_std:.3f} "
                            f"delta={delta_mean:+.3f}±{delta_std:.3f} (n={n_splits})"
                        )
                else:
                    if console:
                        lines.append(
                            f"[{component}] {target_label} | {model_name}: "
                            f"baseline R^2={base_mean:.3f}, augmented R^2={aug_mean:.3f}, "
                            f"delta={delta_mean:+.3f}"
                        )
                    else:
                        lines.append(
                            f"    {model_name:<14} baseline={base_mean:.3f} "
                            f"augmented={aug_mean:.3f} delta={delta_mean:+.3f}"
                        )
    return lines


GROUP_METRIC_SIGN = {
    "group_mean_r2": 1.0,
    "group_mean_spearman": 1.0,
    "group_balanced_mse": -1.0,  # lower is better
}


def format_group_metric_lines(
    component: str, metrics_df: pd.DataFrame, console: bool
) -> List[str]:
    if metrics_df.empty:
        return []
    split_families = (
        sorted(metrics_df["split_family"].dropna().unique().tolist())
        if "split_family" in metrics_df.columns
        else ["row"]
    )
    metric_names = sorted(metrics_df["metric"].dropna().unique().tolist())
    lines: List[str] = []
    for split_family in split_families:
        df_split = metrics_df[metrics_df["split_family"] == split_family]
        header = (
            f"\n=== Component: {component} | split={split_family} | group-metrics ==="
            if console
            else f"Component: {component} | split={split_family} | group-metrics"
        )
        lines.append(header)
        for metric_name in metric_names:
            lines.append(f"  Metric: {metric_name}")
            sign = GROUP_METRIC_SIGN.get(metric_name, 1.0)
            for _, target_label in TARGETS:
                df_target = df_split[
                    (df_split["target_label"] == target_label)
                    & (df_split["metric"] == metric_name)
                ]
                if df_target.empty:
                    continue
                for model_name in MODEL_BUILDERS:
                    df_model = df_target[df_target["model"] == model_name]
                    if df_model.empty:
                        continue
                    pivot = df_model.pivot_table(
                        index="split",
                        columns="feature_set",
                        values="value",
                        aggfunc="mean",
                    )
                    base_vals = pivot.get("baseline")
                    aug_vals = pivot.get("augmented")
                    if base_vals is None or aug_vals is None:
                        continue
                    delta_vals = (aug_vals - base_vals) * sign
                    n_splits = int(delta_vals.dropna().shape[0])
                    base_mean = float(base_vals.mean())
                    aug_mean = float(aug_vals.mean())
                    delta_mean = float(delta_vals.mean())
                    if n_splits > 1:
                        base_std = float(base_vals.std(ddof=0))
                        aug_std = float(aug_vals.std(ddof=0))
                        delta_std = float(delta_vals.std(ddof=0))
                        if console:
                            lines.append(
                                f"[{component}] {target_label} | {model_name}: "
                                f"baseline={base_mean:.3f}±{base_std:.3f}, "
                                f"augmented={aug_mean:.3f}±{aug_std:.3f}, "
                                f"delta={delta_mean:+.3f}±{delta_std:.3f} (n={n_splits})"
                            )
                        else:
                            lines.append(
                                f"    {target_label} | {model_name:<14} "
                                f"baseline={base_mean:.3f}±{base_std:.3f} "
                                f"augmented={aug_mean:.3f}±{aug_std:.3f} "
                                f"delta={delta_mean:+.3f}±{delta_std:.3f} (n={n_splits})"
                            )
                    else:
                        if console:
                            lines.append(
                                f"[{component}] {target_label} | {model_name}: "
                                f"baseline={base_mean:.3f}, augmented={aug_mean:.3f}, "
                                f"delta={delta_mean:+.3f}"
                            )
                        else:
                            lines.append(
                                f"    {target_label} | {model_name:<14} "
                                f"baseline={base_mean:.3f} augmented={aug_mean:.3f} "
                                f"delta={delta_mean:+.3f}"
                            )
    return lines


def format_permutation_lines(
    component: str,
    perm_df: pd.DataFrame,
    r2_df: pd.DataFrame,
    group_df: pd.DataFrame,
    console: bool,
) -> List[str]:
    if perm_df.empty:
        return []
    split_families = sorted(
        perm_df["split_family"].dropna().unique().tolist()
    )
    metric_names = sorted(perm_df["metric"].dropna().unique().tolist())
    lines: List[str] = []
    for split_family in split_families:
        df_split_perm = perm_df[perm_df["split_family"] == split_family]
        header = (
            f"\n=== Component: {component} | split={split_family} | perm-null ==="
            if console
            else f"Component: {component} | split={split_family} | perm-null"
        )
        lines.append(header)
        for metric_name in metric_names:
            lines.append(f"  Metric: {metric_name}")
            sign = GROUP_METRIC_SIGN.get(metric_name, 1.0)
            for _, target_label in TARGETS:
                for model_name in MODEL_BUILDERS:
                    null_sub = df_split_perm[
                        (df_split_perm["target_label"] == target_label)
                        & (df_split_perm["model"] == model_name)
                        & (df_split_perm["metric"] == metric_name)
                    ]
                    if null_sub.empty:
                        continue
                    if metric_name == "row_r2":
                        actual_sub = r2_df[
                            (r2_df["target_label"] == target_label)
                            & (r2_df["model"] == model_name)
                            & (r2_df["split_family"] == split_family)
                        ]
                        pivot = actual_sub.pivot_table(
                            index="split",
                            columns="feature_set",
                            values="r2",
                            aggfunc="mean",
                        )
                        base_vals = pivot.get("baseline")
                        aug_vals = pivot.get("augmented")
                        if base_vals is None or aug_vals is None:
                            continue
                        actual_deltas = (aug_vals - base_vals) * sign
                    else:
                        actual_sub = group_df[
                            (group_df["target_label"] == target_label)
                            & (group_df["model"] == model_name)
                            & (group_df["metric"] == metric_name)
                            & (group_df["split_family"] == split_family)
                        ]
                        pivot = actual_sub.pivot_table(
                            index="split",
                            columns="feature_set",
                            values="value",
                            aggfunc="mean",
                        )
                        base_vals = pivot.get("baseline")
                        aug_vals = pivot.get("augmented")
                        if base_vals is None or aug_vals is None:
                            continue
                        actual_deltas = (aug_vals - base_vals) * sign

                    actual_median = float(np.nanmedian(actual_deltas))
                    actual_mean = float(np.nanmean(actual_deltas))
                    null_deltas = null_sub["delta"].to_numpy(dtype=float)
                    null_median = float(np.nanmedian(null_deltas))
                    null_mean = float(np.nanmean(null_deltas))
                    p_value = float(
                        np.mean(null_deltas >= actual_median)
                        if null_deltas.size
                        else float("nan")
                    )
                    if console:
                        lines.append(
                            f"[{component}] {target_label} | {model_name}: "
                            f"actual median={actual_median:+.3f} mean={actual_mean:+.3f}; "
                            f"null median={null_median:+.3f} mean={null_mean:+.3f}; "
                            f"p≈{p_value:.3f}"
                        )
                    else:
                        lines.append(
                            f"    {target_label} | {model_name:<14} "
                            f"actual_med={actual_median:+.3f} actual_mean={actual_mean:+.3f} "
                            f"null_med={null_median:+.3f} null_mean={null_mean:+.3f} "
                            f"p≈{p_value:.3f}"
                        )
    return lines


def summarize_component(component: str, results: pd.DataFrame) -> None:
    for line in format_component_lines(component, results, console=True):
        print(line)


def format_feature_ranking(
    model_name: str,
    model: Any,
    feature_cols: Sequence[str],
    top_features: int,
) -> List[str]:
    n_features = len(feature_cols)
    limit = n_features if top_features <= 0 else min(top_features, n_features)
    lines: List[str] = []
    if model_name == "Ridge" and hasattr(model, "coef_"):
        coefs = np.asarray(model.coef_).ravel()
        order = np.argsort(np.abs(coefs))[::-1]
        lines.append(
            f"      coefficients (sorted by |coef|, top {limit} of {n_features}):"
        )
        for idx in order[:limit]:
            lines.append(f"        {feature_cols[idx]:<30} {coefs[idx]:+0.4f}")
    elif hasattr(model, "feature_importances_"):
        importances = np.asarray(model.feature_importances_).ravel()
        order = np.argsort(importances)[::-1]
        lines.append(
            f"      feature importances (top {limit} of {n_features}):"
        )
        for idx in order[:limit]:
            lines.append(f"        {feature_cols[idx]:<30} {importances[idx]:0.4f}")
    else:
        lines.append("      (model details unavailable for this estimator)")
    return lines


def format_model_detail(
    target_label: str,
    model_name: str,
    feature_set: str,
    feature_cols: Sequence[str],
    r2: float,
    model: Any,
    n_train: int,
    n_test: int,
    top_features: int,
    split_name: str,
) -> List[str]:
    lines = [
        f"      {model_name} [{feature_set}] split={split_name} r2={r2:.3f} "
        f"(train={n_train}, test={n_test}, features={len(feature_cols)})",
        f"        features: {', '.join(feature_cols)}",
    ]
    if model_name == "Ridge" and hasattr(model, "intercept_"):
        try:
            intercept_val = float(np.asarray(model.intercept_).ravel()[0])
            lines.append(f"        intercept: {intercept_val:+0.4f}")
        except Exception:
            lines.append("        intercept: (unavailable)")
    lines.extend(format_feature_ranking(model_name, model, feature_cols, top_features))
    return lines


def summarize_sample_id_distribution(
    df_comp: pd.DataFrame, component: str, top_k: int = 5
) -> List[str]:
    if "sample_id" not in df_comp.columns or df_comp.empty:
        return [f"[{component}] sample_id stats: unavailable."]
    counts = (
        df_comp.groupby("sample_id")
        .size()
        .sort_values(ascending=False)
        .astype(int)
    )
    n_samples = int(counts.size)
    total_rows = int(len(df_comp))
    mean_rows = float(counts.mean()) if n_samples else 0.0
    median_rows = float(counts.median()) if n_samples else 0.0
    min_rows = int(counts.min()) if n_samples else 0
    max_rows = int(counts.max()) if n_samples else 0
    top_items = counts.head(top_k).items()
    top_str = ", ".join(f"{sid}({cnt})" for sid, cnt in top_items)
    return [
        f"[{component}] sample_id stats: {n_samples} unique samples, {total_rows} rows.",
        (
            f"[{component}] rows/sample_id: mean={mean_rows:.2f}, "
            f"median={median_rows:.1f}, min={min_rows}, max={max_rows}."
        ),
        f"[{component}] top {top_k} sample_ids by rows: {top_str}.",
    ]


def iter_split_indices(
    df_target: pd.DataFrame,
    component: str,
    split_mode: str,
    group_n_splits: int,
) -> List[Tuple[str, np.ndarray, np.ndarray]]:
    splits: List[Tuple[str, np.ndarray, np.ndarray]] = []
    if split_mode in ("row", "both"):
        train_idx, test_idx = train_test_split(
            df_target.index, test_size=TEST_SIZE, random_state=RANDOM_STATE
        )
        splits.append(("row", train_idx, test_idx))

    if split_mode in ("group", "both"):
        groups = df_target["sample_id"]
        n_groups = int(groups.nunique(dropna=True))
        if n_groups < 2:
            print(
                f"[{component}] Not enough unique sample_id ({n_groups}) for group split; skipping."
            )
        else:
            gss = GroupShuffleSplit(
                n_splits=max(1, group_n_splits),
                test_size=TEST_SIZE,
                random_state=RANDOM_STATE,
            )
            for idx, (train_pos, test_pos) in enumerate(
                gss.split(df_target, groups=groups)
            ):
                split_name = (
                    f"group{idx + 1}" if group_n_splits > 1 else "group"
                )
                splits.append(
                    (
                        split_name,
                        df_target.index[train_pos],
                        df_target.index[test_pos],
                    )
                )
    return splits


def experiment_settings_line(args: argparse.Namespace) -> str:
    comps = args.components if args.components else "all"
    return (
        "Settings: "
        f"report_details={args.report_details}, top_features={args.top_features}, "
        f"split_mode={args.split_mode}, group_n_splits={args.group_n_splits}, "
        f"baseline_augment={args.baseline_augment}, esm_pca_components={args.esm_pca_components}, "
        f"standardize_linear={args.standardize_linear}, group_metrics={args.group_metrics}, "
        f"permute_esm_n={args.permute_esm_n}, "
        f"tree(n_estimators={args.tree_n_estimators}, min_samples_leaf={args.tree_min_samples_leaf}, "
        f"max_depth={args.tree_max_depth}, extra_trees={args.include_extra_trees}), "
        f"components={comps}"
    )


def run_experiment(
    args: argparse.Namespace,
    df_features: pd.DataFrame,
    df_esm: pd.DataFrame,
    baseline_feature_candidates: Sequence[str],
    title: str | None = None,
    description: str | None = None,
) -> List[str]:
    os.makedirs(args.outdir, exist_ok=True)
    global MODEL_BUILDERS
    MODEL_BUILDERS = make_model_builders(args)

    df = df_features.merge(df_esm, on="sample_id", how="left")
    df["log_runtime_sec"] = np.log(df["runtime_sec"])
    df["log_peak_memory_mib"] = np.log(df["peak_memory_mib"])

    components = sorted(df["component"].dropna().unique())
    if args.components:
        requested = set(args.components)
        components = [c for c in components if c in requested]

    report_lines: List[str] = []
    if title:
        header_lines = [
            "",
            f"## {title}",
            description or "",
            experiment_settings_line(args),
            "",
        ]
        for line in header_lines:
            if line:
                print(line)
                report_lines.append(line)
            else:
                print("")
                report_lines.append("")

    print(f"Merged features: {len(df)} rows across {len(components)} components.")
    report_lines.append(
        f"Merged features: {len(df)} rows across {len(components)} components."
    )

    all_results: List[Dict[str, object]] = []
    all_group_metrics: List[Dict[str, object]] = []
    perm_results: List[Dict[str, object]] = []

    for component in components:
        component_detail_lines: List[str] = []
        df_comp = df[df["component"] == component].copy()
        df_comp = df_comp.dropna(
            subset=[
                "runtime_sec",
                "peak_memory_mib",
                "mean_temporal_util_percent",
                "log_runtime_sec",
                "log_peak_memory_mib",
                *ESM_FEATURE_COLUMNS,
            ]
        )
        if len(df_comp) < args.min_rows:
            print(f"[{component}] Skipping: only {len(df_comp)} rows after NA drop.")
            continue

        baseline_cols = filter_features_by_nonnull(
            df_comp, baseline_feature_candidates, args.min_nonnull_frac
        )
        if not baseline_cols:
            print(f"[{component}] Skipping: no baseline numeric features left.")
            continue

        df_comp = df_comp.dropna(subset=baseline_cols)
        if len(df_comp) < args.min_rows:
            print(
                f"[{component}] Skipping: only {len(df_comp)} rows after dropping NA in baseline features."
            )
            continue

        derived_cols = add_derived_baseline_features(
            df_comp, baseline_cols, args.baseline_augment
        )
        if derived_cols:
            derived_cols = filter_features_by_nonnull(
                df_comp, derived_cols, args.min_nonnull_frac
            )
            if derived_cols:
                baseline_cols = baseline_cols + derived_cols

        if args.pca_analysis and args.esm_pca_components > 0:
            for line in run_pca_analysis(
                component, df_comp, baseline_cols, args.esm_pca_components
            ):
                print(line)
                report_lines.append(line.replace(f"[{component}] ", "  "))

        for line in summarize_sample_id_distribution(df_comp, component):
            print(line)
            report_lines.append(line.replace(f"[{component}] ", "  "))

        for target_col, target_label in TARGETS:
            if target_col not in df_comp.columns:
                print(f"[{component}] Missing target {target_col}; skipping.")
                continue
            df_target = df_comp.dropna(subset=[target_col])
            if len(df_target) < args.min_rows:
                print(
                    f"[{component}] Target {target_col}: only {len(df_target)} rows; skipping."
                )
                continue

            if args.report_details:
                component_detail_lines.append(f"    {target_label}:")
            X_base = df_target[baseline_cols]
            X_aug = df_target[baseline_cols + ESM_FEATURE_COLUMNS]
            y = df_target[target_col]

            split_indices = iter_split_indices(
                df_target, component, args.split_mode, args.group_n_splits
            )
            for split_name, train_idx, test_idx in split_indices:
                split_family = (
                    "group" if split_name.startswith("group") else split_name
                )
                X_train_base = X_base.loc[train_idx]
                X_test_base = X_base.loc[test_idx]
                if args.esm_pca_components > 0:
                    scaler = StandardScaler()
                    train_esm = df_target.loc[
                        train_idx, ESM_FEATURE_COLUMNS
                    ].to_numpy()
                    test_esm = df_target.loc[
                        test_idx, ESM_FEATURE_COLUMNS
                    ].to_numpy()
                    n_components = min(
                        args.esm_pca_components,
                        len(ESM_FEATURE_COLUMNS),
                        train_esm.shape[0],
                    )
                    train_scaled = scaler.fit_transform(train_esm)
                    test_scaled = scaler.transform(test_esm)
                    pca = PCA(
                        n_components=n_components, random_state=RANDOM_STATE
                    )
                    train_pca = pca.fit_transform(train_scaled)
                    test_pca = pca.transform(test_scaled)
                    pca_cols = [
                        f"esm_pca{i + 1}" for i in range(n_components)
                    ]
                    X_train_pca = pd.DataFrame(
                        train_pca, index=train_idx, columns=pca_cols
                    )
                    X_test_pca = pd.DataFrame(
                        test_pca, index=test_idx, columns=pca_cols
                    )
                    X_train_aug = pd.concat(
                        [X_train_base, X_train_pca], axis=1
                    )
                    X_test_aug = pd.concat(
                        [X_test_base, X_test_pca], axis=1
                    )
                    aug_feature_cols = baseline_cols + pca_cols
                else:
                    X_train_aug = X_aug.loc[train_idx]
                    X_test_aug = X_aug.loc[test_idx]
                    aug_feature_cols = baseline_cols + ESM_FEATURE_COLUMNS
                y_train = y.loc[train_idx]
                y_test = y.loc[test_idx]

                for model_name in MODEL_BUILDERS:
                    r2_base, model_base = train_and_score_model(
                        model_name, X_train_base, y_train, X_test_base, y_test
                    )
                    r2_aug, model_aug = train_and_score_model(
                        model_name, X_train_aug, y_train, X_test_aug, y_test
                    )
                    all_results.append(
                        {
                            "component": component,
                            "target_col": target_col,
                            "target_label": target_label,
                            "model": model_name,
                            "feature_set": "baseline",
                            "r2": r2_base,
                            "split": split_name,
                            "split_family": split_family,
                        }
                    )
                    all_results.append(
                        {
                            "component": component,
                            "target_col": target_col,
                            "target_label": target_label,
                            "model": model_name,
                            "feature_set": "augmented",
                            "r2": r2_aug,
                            "split": split_name,
                            "split_family": split_family,
                        }
                    )
                    if args.group_metrics:
                        groups_test = df_target.loc[test_idx, "sample_id"]
                        pred_base = np.asarray(model_base.predict(X_test_base)).ravel()
                        pred_aug = np.asarray(model_aug.predict(X_test_aug)).ravel()
                        base_metrics = compute_group_metrics(
                            y_test, pred_base, groups_test
                        )
                        aug_metrics = compute_group_metrics(
                            y_test, pred_aug, groups_test
                        )
                        for metric_name, metric_val in base_metrics.items():
                            all_group_metrics.append(
                                {
                                    "component": component,
                                    "target_col": target_col,
                                    "target_label": target_label,
                                    "model": model_name,
                                    "feature_set": "baseline",
                                    "metric": metric_name,
                                    "value": metric_val,
                                    "split": split_name,
                                    "split_family": split_family,
                                }
                            )
                        for metric_name, metric_val in aug_metrics.items():
                            all_group_metrics.append(
                                {
                                    "component": component,
                                    "target_col": target_col,
                                    "target_label": target_label,
                                    "model": model_name,
                                    "feature_set": "augmented",
                                    "metric": metric_name,
                                    "value": metric_val,
                                    "split": split_name,
                                    "split_family": split_family,
                                }
                            )
                    if args.permute_esm_n > 0:
                        rng = np.random.default_rng(args.permute_seed)
                        train_esm_raw = df_target.loc[
                            train_idx, ESM_FEATURE_COLUMNS
                        ].to_numpy()
                        if args.group_metrics:
                            groups_test = df_target.loc[test_idx, "sample_id"]
                            base_metrics_null = compute_group_metrics(
                                y_test,
                                np.asarray(model_base.predict(X_test_base)).ravel(),
                                groups_test,
                            )
                        for perm_idx in range(args.permute_esm_n):
                            permuted_esm = rng.permutation(train_esm_raw)
                            if args.esm_pca_components > 0:
                                test_esm_raw = df_target.loc[
                                    test_idx, ESM_FEATURE_COLUMNS
                                ].to_numpy()
                                n_components_null = min(
                                    args.esm_pca_components,
                                    len(ESM_FEATURE_COLUMNS),
                                    permuted_esm.shape[0],
                                )
                                scaler_null = StandardScaler()
                                train_scaled_null = scaler_null.fit_transform(
                                    permuted_esm
                                )
                                test_scaled_null = scaler_null.transform(
                                    test_esm_raw
                                )
                                pca_null = PCA(
                                    n_components=n_components_null,
                                    random_state=RANDOM_STATE,
                                )
                                train_pca_null = pca_null.fit_transform(
                                    train_scaled_null
                                )
                                test_pca_null = pca_null.transform(
                                    test_scaled_null
                                )
                                pca_cols_null = [
                                    f"esm_pca{i + 1}"
                                    for i in range(n_components_null)
                                ]
                                X_train_null = pd.concat(
                                    [
                                        X_train_base,
                                        pd.DataFrame(
                                            train_pca_null,
                                            index=train_idx,
                                            columns=pca_cols_null,
                                        ),
                                    ],
                                    axis=1,
                                )
                                X_test_null = pd.concat(
                                    [
                                        X_test_base,
                                        pd.DataFrame(
                                            test_pca_null,
                                            index=test_idx,
                                            columns=pca_cols_null,
                                        ),
                                    ],
                                    axis=1,
                                )
                            else:
                                perm_df = pd.DataFrame(
                                    permuted_esm,
                                    index=train_idx,
                                    columns=ESM_FEATURE_COLUMNS,
                                )
                                X_train_null = pd.concat(
                                    [X_train_base, perm_df], axis=1
                                )
                                X_test_null = X_test_aug

                            model_null = MODEL_BUILDERS[model_name]()
                            model_null.fit(X_train_null, y_train)
                            r2_null = float(
                                model_null.score(X_test_null, y_test)
                            )
                            perm_results.append(
                                {
                                    "component": component,
                                    "target_col": target_col,
                                    "target_label": target_label,
                                    "model": model_name,
                                    "metric": "row_r2",
                                    "delta": r2_null - r2_base,
                                    "perm_idx": perm_idx,
                                    "split": split_name,
                                    "split_family": split_family,
                                }
                            )
                            if args.group_metrics:
                                pred_null = np.asarray(
                                    model_null.predict(X_test_null)
                                ).ravel()
                                null_metrics = compute_group_metrics(
                                    y_test, pred_null, groups_test
                                )
                                for metric_name, metric_val in (
                                    null_metrics.items()
                                ):
                                    sign = GROUP_METRIC_SIGN.get(
                                        metric_name, 1.0
                                    )
                                    base_val = base_metrics_null.get(
                                        metric_name, float("nan")
                                    )
                                    perm_results.append(
                                        {
                                            "component": component,
                                            "target_col": target_col,
                                            "target_label": target_label,
                                            "model": model_name,
                                            "metric": metric_name,
                                            "delta": (metric_val - base_val)
                                            * sign,
                                            "perm_idx": perm_idx,
                                            "split": split_name,
                                            "split_family": split_family,
                                        }
                                    )
                    delta = r2_aug - r2_base
                    if args.verbose:
                        print(
                            f"[{component}] {target_label} | {model_name} | split={split_name}: "
                            f"baseline R^2={r2_base:.3f}, augmented R^2={r2_aug:.3f}, delta={delta:+.3f}"
                        )
                    if args.report_details:
                        component_detail_lines.extend(
                            format_model_detail(
                                target_label,
                                model_name,
                                "baseline",
                                baseline_cols,
                                r2_base,
                                model_base,
                                len(train_idx),
                                len(test_idx),
                                args.top_features,
                                split_name,
                            )
                        )
                        component_detail_lines.extend(
                            format_model_detail(
                                target_label,
                                model_name,
                                "augmented",
                                aug_feature_cols,
                                r2_aug,
                                model_aug,
                                len(train_idx),
                                len(test_idx),
                                args.top_features,
                                split_name,
                            )
                        )

        component_results = pd.DataFrame(
            [row for row in all_results if row["component"] == component]
        )
        if not component_results.empty:
            summarize_component(component, component_results)
            component_report = format_component_lines(
                component, component_results, console=False
            )
            if args.report_details and component_detail_lines:
                component_report.append("  Details:")
                component_report.extend(component_detail_lines)
            report_lines.extend(component_report)
            report_lines.append("")
            if args.group_metrics:
                component_metrics = pd.DataFrame(
                    [
                        row
                        for row in all_group_metrics
                        if row["component"] == component
                    ]
                )
                if not component_metrics.empty:
                    for line in format_group_metric_lines(
                        component, component_metrics, console=True
                    ):
                        print(line)
                    metrics_report = format_group_metric_lines(
                        component, component_metrics, console=False
                    )
                    report_lines.extend(metrics_report)
                    report_lines.append("")
            if args.permute_esm_n > 0:
                component_perm = pd.DataFrame(
                    [
                        row
                        for row in perm_results
                        if row["component"] == component
                    ]
                )
                if not component_perm.empty:
                    group_df_for_perm = (
                        component_metrics
                        if args.group_metrics
                        else pd.DataFrame()
                    )
                    for line in format_permutation_lines(
                        component,
                        component_perm,
                        component_results,
                        group_df_for_perm,
                        console=True,
                    ):
                        print(line)
                    perm_report = format_permutation_lines(
                        component,
                        component_perm,
                        component_results,
                        group_df_for_perm,
                        console=False,
                    )
                    report_lines.extend(perm_report)
                    report_lines.append("")

            split_families = (
                sorted(component_results["split_family"].dropna().unique().tolist())
                if "split_family" in component_results.columns
                else (
                    sorted(component_results["split"].dropna().unique().tolist())
                    if "split" in component_results.columns
                    else ["row"]
                )
            )
            for split_family in split_families:
                plot_r2_comparison(
                    component, component_results, args.outdir, split_family
                )

    if not all_results:
        print("No components produced results; check input coverage and settings.")
    else:
        print(
            f"Completed ESM feature comparison for {len(set(r['component'] for r in all_results))} components."
        )
    return report_lines


def main() -> None:
    args = parse_args()
    args.report_details = resolve_report_details(args)
    df_features = load_baseline_features(args.features_csv)
    df_esm = pd.read_csv(args.esm_csv)
    baseline_feature_candidates = select_baseline_feature_columns(df_features)

    if args.run_all_experiments:
        combined_lines: List[str] = []
        combined_lines.extend(ALL_EXPERIMENTS_INTRO.strip().splitlines())
        combined_lines.append("")
        for preset in build_all_experiment_presets(args):
            exp_args = argparse.Namespace(**vars(args))
            for key, val in preset["overrides"].items():
                setattr(exp_args, key, val)
            exp_args.outdir = preset["outdir"]
            exp_lines = run_experiment(
                exp_args,
                df_features,
                df_esm,
                baseline_feature_candidates,
                title=str(preset["title"]),
                description=str(preset["description"]),
            )
            combined_lines.extend(exp_lines)
            combined_lines.append("")

        if combined_lines and args.report_txt:
            report_path = Path(args.report_txt)
            report_path.parent.mkdir(parents=True, exist_ok=True)
            report_path.write_text("\n".join(combined_lines).rstrip() + "\n")
            print(f"Wrote combined experiment report to {report_path}")
    else:
        report_lines = run_experiment(
            args, df_features, df_esm, baseline_feature_candidates
        )
        if report_lines and args.report_txt:
            report_path = Path(args.report_txt)
            report_path.parent.mkdir(parents=True, exist_ok=True)
            report_path.write_text("\n".join(report_lines).rstrip() + "\n")
            print(f"Wrote fit summary to {report_path}")


if __name__ == "__main__":
    main()
